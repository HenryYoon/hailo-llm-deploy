# %%
# notebooks/01_stage1_legal_expert.ipynb

"""
=================================================
Stage 1: Legal Expert Fine-tuning
- ëª©í‘œ: ë²•ë¥  ì „ë¬¸ ì§€ì‹ í•™ìŠµ
- ë°ì´í„°: 16.5K ìƒ˜í”Œ
- ì˜ˆìƒ ì‹œê°„: ~18ì‹œê°„ (RTX 3060 12GB)
=================================================
"""

# ========================================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ========================================
import torch
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer, SFTConfig

# ========================================
# 2. ì„¤ì •ê°’
# ========================================
# ëª¨ë¸ ì„¤ì •
max_seq_length = 1024
dtype = None  # Auto-detect (BF16 for Ampere+)
load_in_4bit = True
seed = 3407

# LoRA ì„¤ì •
lora_r = 16
lora_alpha = 16
lora_dropout = 0
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                  "gate_proj", "up_proj", "down_proj"]

# í•™ìŠµ ì„¤ì •
output_dir = "../../models/checkpoints/stage1"
num_train_epochs = 1
per_device_train_batch_size = 2
gradient_accumulation_steps = 1
learning_rate = 2e-4
logging_steps = 10

# ë°ì´í„° ê²½ë¡œ
train_data_path = "../../data/processed/trial1/train_dataset_trial1.json"
val_data_path = "../../data/processed/trial1/val_dataset_trial1.json"

model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "Qwen/Qwen2.5-3B-Instruct",
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
        device_map="balanced",
    )

model = FastLanguageModel.get_peft_model(
        model,
        r = lora_r,
        target_modules = target_modules,
        lora_alpha = lora_alpha,
        lora_dropout = lora_dropout,
        bias = "none",
        use_gradient_checkpointing = "unsloth",  # 30% ë” ë¹ ë¥¸ ì²´í¬í¬ì¸íŒ…
        random_state = seed,
        use_rslora = False,  # Rank-Stabilized LoRA
        loftq_config = None,
    )

if __name__ == "__main__":

    # ========================================
    # 3. ëª¨ë¸ ë¡œë“œ
    # ========================================
    print("ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...")

    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: Qwen2.5-7B-Instruct")
    print(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

    # ========================================
    # 4. LoRA ì„¤ì •
    # ========================================
    print("ğŸ”§ LoRA ì„¤ì • ì¤‘...")

    

    print(f"âœ… LoRA ì„¤ì • ì™„ë£Œ (r={lora_r}, alpha={lora_alpha})")

    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)")

    # ========================================
    # 5. ë°ì´í„°ì…‹ ì¤€ë¹„
    # ========================================
    print("ğŸ“š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")

    dataset = load_dataset("json", data_files={
        "train": train_data_path,
        "validation": val_data_path
    })

    print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
    print(f"  - Train: {len(dataset['train'])} ìƒ˜í”Œ")
    print(f"  - Validation: {len(dataset['validation'])} ìƒ˜í”Œ")

    # ìƒ˜í”Œ í™•ì¸
    print("\nğŸ“ ë°ì´í„° ìƒ˜í”Œ (ì²« ë²ˆì§¸):")
    print(dataset['train'][0])

    # ========================================
    # 6. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
    # ========================================
    # ChatML í¬ë§·
    

    def formatting_prompts_func(examples):
        alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

        ### Instruction:
        {}

        ### Input:
        {}

        ### Response:
        {}"""
        """ë°ì´í„°ë¥¼ í•™ìŠµ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        instructions = examples["instruction"]
        inputs = examples["input"] if "input" in examples else [""] * len(instructions)
        outputs = examples["output"]
        texts = []
        
        for instruction, input_text, output in zip(instructions, inputs, outputs):
            # inputì´ ì—†ìœ¼ë©´ ë‹¨ìˆœí™”
            if input_text.strip() == "":
                text = f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

                ### Instruction:
                {instruction}

                ### Response:
                {output}"""
            else:
                text = alpaca_prompt.format(instruction, input_text, output)
            
            text += tokenizer.eos_token
            texts.append(text)
        
        return {"text": texts}

    # ë°ì´í„°ì…‹ ë³€í™˜
    dataset = dataset.map(formatting_prompts_func, batched=True)

    print("âœ… í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… ì™„ë£Œ")


    # ========================================
    # 7. Trainer ì„¤ì •
    # ========================================
    print("ğŸ‹ï¸ Trainer ì„¤ì • ì¤‘...")

    trainer = SFTTrainer(
        model = model,
        tokenizer = tokenizer,
        train_dataset = dataset["train"],
        eval_dataset = dataset["validation"],
        dataset_text_field = "text",
        max_seq_length = max_seq_length,
        packing = False,  # Stage 1ì—ì„œëŠ” False
        args = SFTConfig(
            per_device_train_batch_size = per_device_train_batch_size,
            gradient_accumulation_steps = gradient_accumulation_steps,
            warmup_steps = 10,
            num_train_epochs = num_train_epochs, # Set this for 1 full training run.
            learning_rate = learning_rate,
            logging_steps = logging_steps,
            optim = "adamw_8bit",
            weight_decay = 0.01,
            lr_scheduler_type = "linear",
            seed = seed,
            output_dir = output_dir,
            report_to = "tensorboard", # Use TrackIO/WandB etc
            eval_strategy='steps',
            eval_steps = 1000,
            save_strategy ='best',
            dataloader_num_workers = 8,   # ì¤‘ìš” (ìœˆë„ìš°ì—ì„œ ì›Œì»¤=spawn)
            load_best_model_at_end=True
        ),
        )


    print("âœ… Trainer ì¤€ë¹„ ì™„ë£Œ")

    # ========================================
    # 8. í•™ìŠµ ì‹œì‘
    # ========================================
    print("\n" + "="*50)
    print("ğŸš€ Stage 1 í•™ìŠµ ì‹œì‘!")
    print("="*50)
    print(f"ğŸ“Š ì´ í•™ìŠµ ìŠ¤í…: {len(dataset['train']) // (per_device_train_batch_size * gradient_accumulation_steps) * num_train_epochs}")
    print(f"â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~18ì‹œê°„ (RTX 3060 12GB)")
    print("="*50 + "\n")

    # í•™ìŠµ ì‹¤í–‰
    trainer_stats = trainer.train()

    print("\n" + "="*50)
    print("âœ… Stage 1 í•™ìŠµ ì™„ë£Œ!")
    print("="*50)
    print(f"ğŸ“Š ìµœì¢… ì†ì‹¤(Loss): {trainer_stats.training_loss:.4f}")
    print(f"â±ï¸  ì‹¤ì œ ì†Œìš” ì‹œê°„: {trainer_stats.metrics['train_runtime'] / 3600:.2f}ì‹œê°„")
    print("="*50 + "\n")

    # ========================================
    # 9. ëª¨ë¸ ì €ì¥
    # ========================================
    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

    # LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥
    model.save_pretrained("../../models/lora_adapters/stage1")
    tokenizer.save_pretrained("../../models/lora_adapters/stage1")
    print("âœ… LoRA ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: ../../models/lora_adapters/stage1")

    # 16-bit ë³‘í•© ëª¨ë¸ ì €ì¥
    model.save_pretrained_merged(
        "../../models/merged/stage1_16bit",
        tokenizer,
        save_method = "merged_16bit",
    )
    print("âœ… ë³‘í•© ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ../../models/merged/stage1_16bit")

    # 4-bit GGUF ì €ì¥ (ì„ íƒ)
    # model.save_pretrained_gguf(
    #     "./models/stage1_gguf",
    #     tokenizer,
    #     quantization_method = "q4_k_m"
    # )
    # print("âœ… GGUF ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ./models/stage1_gguf")

    print("\nğŸ‰ Stage 1 ì™„ë£Œ!")




# %%
