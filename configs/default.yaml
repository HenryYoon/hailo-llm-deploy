model:
  name: "Qwen/Qwen2.5-3B-Instruct"
  max_seq_length: 2048
  load_in_4bit: true

lora:
  r: 16
  alpha: 16
  dropout: 0
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

training:
  epochs: 10
  batch_size: 2
  gradient_accumulation: 4
  learning_rate: 2e-4
  warmup_ratio: 0.03
  scheduler: cosine
  weight_decay: 0.01
  eval_steps: 50
  save_steps: 50
  seed: 42
  prompt_format: chatml

data:
  train_path: null
  val_path: null
  test_path: null
  instruction: null

export:
  format: onnx
  dtype: float16
  output_dir: ./output

compile:
  har_path: null
  alls_path: null
  hw_arch: hailo10h
  adapter_name: lora_adapter
  lora_weights_path: null
  calibration_data_path: null
  calibration_size: 64
  output_dir: ./models/compiled
  output_name: model

deploy:
  target: hailo-10h
  port: 8000

evaluate:
  metrics:
    - rouge_l
    - bertscore
  llm_judge: false
  prompt_format: chatml
  max_new_tokens: 512
  temperature: 0.1
  top_p: 0.9
  repetition_penalty: 1.1
